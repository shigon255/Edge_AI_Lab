{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8rFS5-s47wa"
      },
      "source": [
        "# Lab4: Quantize DeiT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XdmT91OIoXZ"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zM6gf9u-99jp"
      },
      "outputs": [],
      "source": [
        "# install the newest version of torch, torchvision, and timm\n",
        "!pip3 uninstall --yes torch torchaudio torchvision torchtext torchdata timm\n",
        "!pip3 install torch torchaudio torchvision torchtext torchdata timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4OYDr727IoXb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/shigon/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "import math\n",
        "import time\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from timm.data import create_transform\n",
        "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torch.export import export, ExportedProgram\n",
        "from torchvision.models.mobilenetv2 import MobileNet_V2_Weights\n",
        "from torch._export import capture_pre_autograd_graph\n",
        "from torch.ao.quantization.quantize_pt2e import convert_pt2e, prepare_pt2e, prepare_qat_pt2e\n",
        "from torch.ao.quantization.quantizer.xnnpack_quantizer import (\n",
        "    get_symmetric_quantization_config,\n",
        "    XNNPACKQuantizer,\n",
        ")\n",
        "\n",
        "# set numpy seed and torch seed\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "torch.cuda.is_available()\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    # model.eval()\n",
        "    model.to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(data_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the model on the test images: {accuracy}%')\n",
        "    return accuracy\n",
        "\n",
        "def print_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "    size = os.path.getsize(\"temp.p\")/1e6\n",
        "    os.remove('temp.p')\n",
        "    return size\n",
        "\n",
        "\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i1jrxEBjIoXc"
      },
      "outputs": [],
      "source": [
        "def data_loader_to_list(data_loader, length=128):\n",
        "    new_data_loader = []\n",
        "    if length < 0:\n",
        "        return list(data_loader)\n",
        "    else:\n",
        "        for i, data in enumerate(data_loader):\n",
        "            if i >= length:\n",
        "                break\n",
        "            new_data_loader.append(data)\n",
        "\n",
        "    return new_data_loader\n",
        "\n",
        "def build_dataset_CIFAR100(is_train, data_path):\n",
        "    transform = build_transform(is_train)\n",
        "    dataset = datasets.CIFAR100(data_path, train=is_train, transform=transform, download=True)\n",
        "    nb_classes = 100\n",
        "    return dataset, nb_classes\n",
        "\n",
        "def build_transform(is_train):\n",
        "    input_size = 224\n",
        "    eval_crop_ratio = 1.0\n",
        "\n",
        "    resize_im = input_size > 32\n",
        "    if is_train:\n",
        "        # this should always dispatch to transforms_imagenet_train\n",
        "        transform = create_transform(\n",
        "            input_size=input_size,\n",
        "            is_training=True,\n",
        "            color_jitter=0.3,\n",
        "            auto_augment='rand-m9-mstd0.5-inc1',\n",
        "            interpolation='bicubic',\n",
        "            re_prob=0.0,\n",
        "            re_mode='pixel',\n",
        "            re_count=1,\n",
        "        )\n",
        "        if not resize_im:\n",
        "            # replace RandomResizedCropAndInterpolation with\n",
        "            # RandomCrop\n",
        "            transform.transforms[0] = transforms.RandomCrop(\n",
        "                input_size, padding=4)\n",
        "        return transform\n",
        "\n",
        "    t = []\n",
        "    if resize_im:\n",
        "        size = int(input_size / eval_crop_ratio)\n",
        "        t.append(\n",
        "            transforms.Resize(size, interpolation=3),  # to maintain same ratio w.r.t. 224 images\n",
        "        )\n",
        "        t.append(transforms.CenterCrop(input_size))\n",
        "\n",
        "    t.append(transforms.ToTensor())\n",
        "    t.append(transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD))\n",
        "    return transforms.Compose(t)\n",
        "\n",
        "def prepare_data(batch_size):\n",
        "    train_set, nb_classes = build_dataset_CIFAR100(is_train=True, data_path='./data')\n",
        "    test_set, _ = build_dataset_CIFAR100(is_train=False, data_path='./data')\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "    return train_loader, test_loader, nb_classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training and calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calibrate(model: nn.Module, data_loader) -> None:\n",
        "    calibration_data = data_loader_to_list(data_loader, math.ceil(128/data_loader.batch_size)) # calibrate 128 images\n",
        "    for image, _ in calibration_data:\n",
        "        model(image)\n",
        "    return\n",
        "\n",
        "def train_one_epoch(model, criterion, optimizer, data_loader, device):\n",
        "    cnt = 0\n",
        "    for image, target in tqdm(data_loader):\n",
        "        cnt += 1\n",
        "        image, target = image.to(device), target.to(device)\n",
        "        output = model(image)\n",
        "        loss = criterion(output, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    model_size = os.path.getsize(\"temp.p\")/1e6\n",
        "    os.remove('temp.p')\n",
        "    return model_size\n",
        "\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    model.to(device)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(data_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the model on the test images: {accuracy}%')\n",
        "    return accuracy\n",
        "\n",
        "def getMiniTestDataset():\n",
        "    # Create a test_loader with batch size = 1\n",
        "    _, test_loader, _ = prepare_data(batch_size=1)\n",
        "\n",
        "    # Prepare to collect 10 images per class\n",
        "    class_images = [[] for _ in range(100)]\n",
        "\n",
        "    # Iterate through the data\n",
        "    for (image, label) in test_loader:\n",
        "        if len(class_images[label]) < 5:\n",
        "            class_images[label].append((image, label))\n",
        "        if all(len(images) == 5 for images in class_images):\n",
        "            break  # Stop once we have 10 images per class\n",
        "\n",
        "    # flatten class_images\n",
        "    mini_test_dataset = []\n",
        "    for images in class_images:\n",
        "        mini_test_dataset.extend(images)\n",
        "    return mini_test_dataset\n",
        "\n",
        "# TA Uses the following code to evaluate your score\n",
        "def lab4_cifar100_evaluation(quantized_model_path='deits_quantized.pth'):\n",
        "    # Prepare data\n",
        "    mini_test_dataset = getMiniTestDataset()\n",
        "\n",
        "    # Load quantized model\n",
        "    quantized_ep = torch.export.load(quantized_model_path)\n",
        "    quantized_model = quantized_ep.module()\n",
        "\n",
        "    # Evaluate model\n",
        "    start_time = time.time()\n",
        "    acc = evaluate_model(quantized_model, mini_test_dataset, device=\"cpu\")\n",
        "    exec_time = time.time() - start_time\n",
        "    model_size = get_size_of_model(quantized_model)\n",
        "\n",
        "    print(f\"Model Size: {model_size:.2f} MB\")\n",
        "    print(f\"Accuracy: {acc:.2f}%\")\n",
        "    print(f\"Execution Time: {exec_time:.2f} s\")\n",
        "\n",
        "    score = 0\n",
        "    if model_size <= 30: score += 10\n",
        "    if model_size <= 27: score += 2 * math.floor(27-model_size)\n",
        "    if acc >= 86:\n",
        "      score += 10 + 2 * math.floor(acc-86)\n",
        "    print(f'Model Score: {score:.2f}')\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkZ8DKOfIoXd"
      },
      "source": [
        "## Part1: Simple Quantization Pipeline (0%)\n",
        "\n",
        "Below is a naive pipeline of quantizing DeiT-S. You may need to modify the pipeline or build your own later on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbo8e-DgIoXd"
      },
      "source": [
        "[**use_reference_representation=False** in **convert_pt2e()** represents fake quant (matmul using fp32).](https://pytorch.org/tutorials/prototype/pt2e_quant_ptq_static.html#convert-the-calibrated-model-to-a-quantized-model)\n",
        "\n",
        "However when the variable is set to True, the execution speed becomes extremely slow.\n",
        "\n",
        "In this lab, it is just fine to set **use_reference_representation=False**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9CfbAo1PIoXe"
      },
      "outputs": [],
      "source": [
        "from torch.export import Dim\n",
        "from torch._export import capture_pre_autograd_graph\n",
        "\n",
        "def quantize_ptq_model(model: nn.Module, data_loader, per_channel=False, quantizer=None) -> None:\n",
        "    # captuer model graph\n",
        "    _dummy_input_data = (next(iter(data_loader))[0],)\n",
        "    model.eval()\n",
        "    dynamic_shapes = {\"x\": {0: Dim(\"batch\")}} # to allow diffent batch size on training/inference\n",
        "    model = capture_pre_autograd_graph(model, _dummy_input_data, dynamic_shapes=dynamic_shapes)\n",
        "\n",
        "    # Init quantizer\n",
        "    if quantizer is None:\n",
        "        quantizer = XNNPACKQuantizer()\n",
        "        quantization_config = get_symmetric_quantization_config(is_per_channel=per_channel, is_qat=False)\n",
        "        quantizer.set_global(quantization_config)\n",
        "\n",
        "    # Prepare model for quantization\n",
        "    model = prepare_pt2e(model, quantizer)\n",
        "\n",
        "    # Calibration\n",
        "    calibrate(model, data_loader)\n",
        "\n",
        "    # Convert model to quantized model\n",
        "    model = convert_pt2e(model, use_reference_representation=False)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Load the model\n",
        "batch_size = 32 # Use batch size > 1 for faster PTQ and QAT.\n",
        "model = torch.load('0.9099_deit3_small_patch16_224.pth', map_location='cpu')\n",
        "train_loader, test_loader, _ = prepare_data(batch_size)\n",
        "\n",
        "# We use only a porpotion of test dataset for evaluation, with batch size = 1 for inference.\n",
        "mini_test_dataset = getMiniTestDataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "I_13WgBMIoXe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:04<00:00, 103.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the test images: 93.6%\n",
            "Size (MB) before quantization: 86.905654\n",
            "Accuracy of the model on the test images: 93.6%\n",
            "Quantizing model...\n",
            "Exporting model...\n",
            "Evaluating model...\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:26<00:00, 18.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the test images: 85.0%\n",
            "Model Size: 21.94 MB\n",
            "Accuracy: 85.00%\n",
            "Execution Time: 26.42 s\n",
            "Model Score: 20.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# evaluate before quantization\n",
        "train_loader, test_loader, _ = prepare_data(batch_size)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "acc = evaluate_model(model, mini_test_dataset, device)\n",
        "print('Size (MB) before quantization:', get_size_of_model(model))\n",
        "print(f'Accuracy of the model on the test images: {acc}%') # 92.8%\n",
        "\n",
        "# quantize model\n",
        "print('Quantizing model...')\n",
        "model.cpu()\n",
        "quantized_model = quantize_ptq_model(model, train_loader, per_channel=False)\n",
        "torch.ao.quantization.move_exported_model_to_eval(quantized_model)\n",
        "\n",
        "print('Exporting model...')\n",
        "quantized_model_path = \"deits_quantized.pth\"\n",
        "\n",
        "quantized_model.cpu()\n",
        "cpu_example_inputs = (torch.randn([1, 3, 224, 224]), ) # batch_size should equal to 1 on inference.\n",
        "quantized_ep = torch.export.export(quantized_model, cpu_example_inputs)\n",
        "torch.export.save(quantized_ep, quantized_model_path)\n",
        "\n",
        "print('Evaluating model...')\n",
        "lab4_cifar100_evaluation(quantized_model_path) # 84.4%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV-fiMUcoO1I"
      },
      "source": [
        "# Below are some tools that may be useful in this lab:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou1NYqlNIoXf"
      },
      "source": [
        "## Profiling Models\n",
        "\n",
        "The code below profiles your model, and generates a trace file for you to find out what is going on in your model throughout the execution.\n",
        "\n",
        "The generated trace file will be named *(test_trace_*.json)*, in the same directory as the notebook. To visualize, go to page [chrome://tracing](chrome://tracing) in either Chrome or Edge browser, then upload the trace file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ha79mAETIoXf"
      },
      "outputs": [],
      "source": [
        "# profile model\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "# ## Default way to use profiler\n",
        "# with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
        "#     for _ in range(10):\n",
        "#         a = torch.square(torch.randn(10000, 10000).cuda())\n",
        "\n",
        "# prof.export_chrome_trace(\"trace.json\")\n",
        "\n",
        "def torch_profile(model, input_data, device):\n",
        "  ## With warmup and skip\n",
        "  # https://pytorch.org/docs/stable/profiler.html\n",
        "\n",
        "  # Non-default profiler schedule allows user to turn profiler on and off\n",
        "  # on different iterations of the training loop;\n",
        "  # trace_handler is called every time a new trace becomes available\n",
        "  def trace_handler(prof):\n",
        "    print(prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=-1))\n",
        "    prof.export_chrome_trace(\"./test_trace_\" + str(prof.step_num) + \".json\")\n",
        "\n",
        "  with torch.profiler.profile(\n",
        "    activities=[\n",
        "      torch.profiler.ProfilerActivity.CPU,\n",
        "      torch.profiler.ProfilerActivity.CUDA,\n",
        "    ],\n",
        "\n",
        "    # In this example with wait=1, warmup=1, active=2, repeat=1,\n",
        "    # profiler will skip the first step/iteration,\n",
        "    # start warming up on the second, record\n",
        "    # the third and the forth iterations,\n",
        "    # after which the trace will become available\n",
        "    # and on_trace_ready (when set) is called;\n",
        "    # the cycle repeats starting with the next step\n",
        "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=2, repeat=1),\n",
        "    on_trace_ready=trace_handler\n",
        "    # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')\n",
        "    # used when outputting for tensorboard\n",
        "    ) as p:\n",
        "      for data in input_data:\n",
        "        model(data.to(device))\n",
        "        # send a signal to the profiler that the next iteration has started\n",
        "        p.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXTFSexrIoXf"
      },
      "outputs": [],
      "source": [
        "dummy_input_data = [next(iter(test_loader))[0] for _ in range(3)]\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# profile quantized model\n",
        "torch_profile(quantized_model, dummy_input_data, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_xk1sSXoqsJ"
      },
      "source": [
        "## To obtain specific model layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "E4BgPWyupMZ1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('blocks.0.attn.qkv', Linear(in_features=384, out_features=1152, bias=True)), ('blocks.0.mlp.fc1', Linear(in_features=384, out_features=1536, bias=True)), ('blocks.0.mlp.fc2', Linear(in_features=1536, out_features=384, bias=True))]\n"
          ]
        }
      ],
      "source": [
        "def match_string(string, match_list, match_prefix=False, match_suffix=False):\n",
        "    if match_prefix:\n",
        "        return any(string.startswith(s) for s in match_list)\n",
        "    elif match_suffix:\n",
        "        return any(string.endswith(s) for s in match_list)\n",
        "    else:\n",
        "        return any(s in string for s in match_list)\n",
        "\n",
        "def get_model_layers(model, match_names=None, match_types=None, prefix=''):\n",
        "    matching_layers = []\n",
        "    for name, module in model.named_modules():\n",
        "        if match_names is None or match_string(name, match_names):\n",
        "            if match_types is None or match_string(type(module).__name__, match_types):\n",
        "                matching_layers.append((f'{prefix}{name}', module))\n",
        "    return matching_layers\n",
        "\n",
        "layer_names = [f'blocks.0.attn.qkv', f'blocks.0.mlp.fc1', f'blocks.0.mlp.fc2']\n",
        "block0_layers = get_model_layers(model, match_names=layer_names, match_types=['Linear'])\n",
        "\n",
        "print(block0_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO1-XuLJBkL2"
      },
      "source": [
        "## To capture the output of each layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "j8xEIdQvBsnQ"
      },
      "outputs": [],
      "source": [
        "class HookHandler:\n",
        "    def __init__(self):\n",
        "        self.handlers = []\n",
        "\n",
        "    def __del__(self):\n",
        "        self.remove_hooks()\n",
        "\n",
        "    def _generate_hook(self, apply_func, key, layer_data_dict: dict):\n",
        "        def hook_fn(model, input, output):\n",
        "            nonlocal layer_data_dict\n",
        "            layer_data_dict[key] = apply_func(\n",
        "                output, layer_data_dict.get(key, None))\n",
        "        return hook_fn\n",
        "\n",
        "    def _bind_hooks(self, apply_layers, apply_func, layer_data_dict: dict):\n",
        "        for name, module in apply_layers:\n",
        "            hook_fn = self._generate_hook(apply_func, name, layer_data_dict)\n",
        "            self.handlers.append(module.register_forward_hook(hook_fn))\n",
        "\n",
        "    def create_hooks(self, apply_layers, apply_func, layer_data_dict: dict):\n",
        "        self._bind_hooks(apply_layers, apply_func, layer_data_dict)\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        for handle in self.handlers:\n",
        "            handle.remove()\n",
        "        self.handlers = []\n",
        "\n",
        "def absmax(data, axis=None):\n",
        "    p_data = data.max(axis=axis)\n",
        "    n_data = data.min(axis=axis)\n",
        "    return np.where(abs(p_data) > abs(n_data), p_data, n_data)\n",
        "\n",
        "def get_flat_act_func(new_val: torch.Tensor, _):\n",
        "    return new_val.cpu().detach().numpy().flatten()\n",
        "\n",
        "\n",
        "def get_act_func(new_val: torch.Tensor, _):\n",
        "    return new_val.cpu().detach().numpy()\n",
        "\n",
        "\n",
        "def get_avg_act_func(new_val: torch.Tensor, existing_val):\n",
        "    if existing_val is None:\n",
        "        avg = new_val.cpu().detach().numpy().sum(axis=0) / total_cnt\n",
        "        total_cnt = new_val.shape[0]\n",
        "\n",
        "    else:\n",
        "        total_cnt, avg = existing_val.get(\"total_cnt\"), existing_val.get(\"value\")\n",
        "        avg = (avg * total_cnt + new_val.cpu().detach().numpy().sum(axis=0)) / (total_cnt + new_val.shape[0])\n",
        "        total_cnt += new_val.shape[0]\n",
        "\n",
        "    return {\"total_cnt\": total_cnt, \"value\": avg}\n",
        "\n",
        "def get_absmax_act_func(new_val: torch.Tensor, existing_val):\n",
        "    new_absmax = absmax(new_val.cpu().detach().numpy(), axis=0)\n",
        "    if existing_val is None:\n",
        "        value = new_absmax\n",
        "    else:\n",
        "        value = absmax(np.stack([existing_val, new_absmax], axis=0), axis=0)\n",
        "    return value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decompose_min_difference(n):\n",
        "    \"\"\"\n",
        "    Decomposes a given number 'n' into two numbers with minimal difference.\n",
        "\n",
        "    Args:\n",
        "        n (int): The input number to be decomposed.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two numbers (x, y) such that x * y = n and abs(x - y) is minimized.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check for valid input\n",
        "    if n <= 1:\n",
        "        raise ValueError(\"Input number must be greater than 1\")\n",
        "\n",
        "    # Initialize variables\n",
        "    min_diff = float('inf')\n",
        "    best_x = None\n",
        "    best_y = None\n",
        "\n",
        "    # Iterate over possible values of x\n",
        "    for x in range(2, int(n**0.5) + 1):\n",
        "        if n % x == 0:\n",
        "            y = n // x\n",
        "            diff = abs(x - y)\n",
        "\n",
        "            # Update best solution if current difference is smaller\n",
        "            if diff < min_diff:\n",
        "                min_diff = diff\n",
        "                best_x = x\n",
        "                best_y = y\n",
        "\n",
        "    # Return the best solution\n",
        "    return best_x, best_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_layer_output_dist(layer_outputs):\n",
        "    # save each layer's outputs as a large image for visualization\n",
        "    import matplotlib.pyplot as plt\n",
        "    import os\n",
        "\n",
        "    # create subplots to accommodate all the layers\n",
        "    num_layer_outputs = len(layer_outputs)\n",
        "    print(num_layer_outputs)\n",
        "    plot_num = decompose_min_difference(num_layer_outputs)\n",
        "    fig, axes = plt.subplots(plot_num[0], plot_num[1], figsize=(plot_num[1]*3, plot_num[0]*3))\n",
        "    axes = axes.ravel()\n",
        "    plot_index = 0\n",
        "\n",
        "    # plot the outputs of each layer\n",
        "    for layer_name, layer_output in layer_outputs.items():\n",
        "        # plot histograms for the outputs of the linear layers\n",
        "        axes[plot_index].hist(layer_output.flatten(), bins=100)\n",
        "        axes[plot_index].set_title(layer_name)\n",
        "        plot_index += 1\n",
        "\n",
        "    fig.tight_layout()\n",
        "    fig.subplots_adjust(top=0.925)\n",
        "    plt.savefig(os.path.join('activation.png'))\n",
        "\n",
        "def plot_layer_output_minmax(layer_outputs):\n",
        "    # plot the min and max values of each layer's output using box plot\n",
        "    import matplotlib.pyplot as plt\n",
        "    import os\n",
        "\n",
        "    flat_layer_outputs = {layer_name: layer_output.flatten() for layer_name, layer_output in layer_outputs.items()}\n",
        "    # draw box plot using test input\n",
        "    fig, ax = plt.subplots()\n",
        "    fig.set_size_inches(10, 30)\n",
        "    ax.boxplot(flat_layer_outputs.values(), vert=False, patch_artist=True, showmeans=True)\n",
        "    ax.set_yticklabels(flat_layer_outputs.keys())\n",
        "    plt.savefig(os.path.join('activation_minmax.png'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['features', 'features.0', 'features.0.0', 'features.0.1', 'features.0.2', 'features.1', 'features.1.conv', 'features.1.conv.0', 'features.1.conv.0.0', 'features.1.conv.0.1', 'features.1.conv.0.2', 'features.1.conv.1', 'features.1.conv.2', 'features.2', 'features.2.conv', 'features.2.conv.0', 'features.2.conv.0.0', 'features.2.conv.0.1', 'features.2.conv.0.2', 'features.2.conv.1', 'features.2.conv.1.0', 'features.2.conv.1.1', 'features.2.conv.1.2', 'features.2.conv.2', 'features.2.conv.3', 'features.3', 'features.3.conv', 'features.3.conv.0', 'features.3.conv.0.0', 'features.3.conv.0.1', 'features.3.conv.0.2', 'features.3.conv.1', 'features.3.conv.1.0', 'features.3.conv.1.1', 'features.3.conv.1.2', 'features.3.conv.2', 'features.3.conv.3', 'features.4', 'features.4.conv', 'features.4.conv.0', 'features.4.conv.0.0', 'features.4.conv.0.1', 'features.4.conv.0.2', 'features.4.conv.1', 'features.4.conv.1.0', 'features.4.conv.1.1', 'features.4.conv.1.2', 'features.4.conv.2', 'features.4.conv.3', 'features.5', 'features.5.conv', 'features.5.conv.0', 'features.5.conv.0.0', 'features.5.conv.0.1', 'features.5.conv.0.2', 'features.5.conv.1', 'features.5.conv.1.0', 'features.5.conv.1.1', 'features.5.conv.1.2', 'features.5.conv.2', 'features.5.conv.3', 'features.6', 'features.6.conv', 'features.6.conv.0', 'features.6.conv.0.0', 'features.6.conv.0.1', 'features.6.conv.0.2', 'features.6.conv.1', 'features.6.conv.1.0', 'features.6.conv.1.1', 'features.6.conv.1.2', 'features.6.conv.2', 'features.6.conv.3', 'features.7', 'features.7.conv', 'features.7.conv.0', 'features.7.conv.0.0', 'features.7.conv.0.1', 'features.7.conv.0.2', 'features.7.conv.1', 'features.7.conv.1.0', 'features.7.conv.1.1', 'features.7.conv.1.2', 'features.7.conv.2', 'features.7.conv.3', 'features.8', 'features.8.conv', 'features.8.conv.0', 'features.8.conv.0.0', 'features.8.conv.0.1', 'features.8.conv.0.2', 'features.8.conv.1', 'features.8.conv.1.0', 'features.8.conv.1.1', 'features.8.conv.1.2', 'features.8.conv.2', 'features.8.conv.3', 'features.9', 'features.9.conv', 'features.9.conv.0', 'features.9.conv.0.0', 'features.9.conv.0.1', 'features.9.conv.0.2', 'features.9.conv.1', 'features.9.conv.1.0', 'features.9.conv.1.1', 'features.9.conv.1.2', 'features.9.conv.2', 'features.9.conv.3', 'features.10', 'features.10.conv', 'features.10.conv.0', 'features.10.conv.0.0', 'features.10.conv.0.1', 'features.10.conv.0.2', 'features.10.conv.1', 'features.10.conv.1.0', 'features.10.conv.1.1', 'features.10.conv.1.2', 'features.10.conv.2', 'features.10.conv.3', 'features.11', 'features.11.conv', 'features.11.conv.0', 'features.11.conv.0.0', 'features.11.conv.0.1', 'features.11.conv.0.2', 'features.11.conv.1', 'features.11.conv.1.0', 'features.11.conv.1.1', 'features.11.conv.1.2', 'features.11.conv.2', 'features.11.conv.3', 'features.12', 'features.12.conv', 'features.12.conv.0', 'features.12.conv.0.0', 'features.12.conv.0.1', 'features.12.conv.0.2', 'features.12.conv.1', 'features.12.conv.1.0', 'features.12.conv.1.1', 'features.12.conv.1.2', 'features.12.conv.2', 'features.12.conv.3', 'features.13', 'features.13.conv', 'features.13.conv.0', 'features.13.conv.0.0', 'features.13.conv.0.1', 'features.13.conv.0.2', 'features.13.conv.1', 'features.13.conv.1.0', 'features.13.conv.1.1', 'features.13.conv.1.2', 'features.13.conv.2', 'features.13.conv.3', 'features.14', 'features.14.conv', 'features.14.conv.0', 'features.14.conv.0.0', 'features.14.conv.0.1', 'features.14.conv.0.2', 'features.14.conv.1', 'features.14.conv.1.0', 'features.14.conv.1.1', 'features.14.conv.1.2', 'features.14.conv.2', 'features.14.conv.3', 'features.15', 'features.15.conv', 'features.15.conv.0', 'features.15.conv.0.0', 'features.15.conv.0.1', 'features.15.conv.0.2', 'features.15.conv.1', 'features.15.conv.1.0', 'features.15.conv.1.1', 'features.15.conv.1.2', 'features.15.conv.2', 'features.15.conv.3', 'features.16', 'features.16.conv', 'features.16.conv.0', 'features.16.conv.0.0', 'features.16.conv.0.1', 'features.16.conv.0.2', 'features.16.conv.1', 'features.16.conv.1.0', 'features.16.conv.1.1', 'features.16.conv.1.2', 'features.16.conv.2', 'features.16.conv.3', 'features.17', 'features.17.conv', 'features.17.conv.0', 'features.17.conv.0.0', 'features.17.conv.0.1', 'features.17.conv.0.2', 'features.17.conv.1', 'features.17.conv.1.0', 'features.17.conv.1.1', 'features.17.conv.1.2', 'features.17.conv.2', 'features.17.conv.3', 'features.18', 'features.18.0', 'features.18.1', 'features.18.2', 'classifier', 'classifier.0', 'classifier.1']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/shigon/miniconda3/envs/executorch/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[25], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (image, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m         \u001b[43mmobilenet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# remove the hooks\u001b[39;00m\n\u001b[1;32m     23\u001b[0m handler\u001b[38;5;241m.\u001b[39mremove_hooks()\n",
            "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.10/site-packages/torchvision/models/mobilenetv2.py:174\u001b[0m, in \u001b[0;36mMobileNetV2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.10/site-packages/torchvision/models/mobilenetv2.py:166\u001b[0m, in \u001b[0;36mMobileNetV2._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# This exists since TorchScript doesn't support inheritance, so the superclass method\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# (this one) needs to have a name other than `forward` that can be accessed in a subclass\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Cannot use \"squeeze\" as batch-size can be 1\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(x, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
            "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.10/site-packages/torchvision/models/mobilenetv2.py:62\u001b[0m, in \u001b[0;36mInvertedResidual.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_res_connect:\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n",
            "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1582\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1580\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1582\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1585\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1586\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1587\u001b[0m     ):\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# initialize HookHandler and the dictionary to store the outputs\n",
        "mobilenet = torch.load('mobilenetv2_0.963.pth')\n",
        "handler = HookHandler()\n",
        "layer_outputs = {}\n",
        "\n",
        "# get the layers you want to record\n",
        "# layer_names = [f'blocks.0.attn.qkv', f'blocks.0.mlp.fc1', f'blocks.0.mlp.fc2']\n",
        "layer_names = [name for name, _ in mobilenet.named_modules()][1:]\n",
        "print(layer_names)\n",
        "block0_layers = get_model_layers(mobilenet, match_names=layer_names, match_types=['Conv2d', 'BatchNorm2d', 'ReLU6', 'Dropout', 'Linear'])\n",
        "\n",
        "# create hooks for the layers (<layers>, <hook function>, <dictionary to store the outputs>)\n",
        "handler.create_hooks(block0_layers, get_act_func, layer_outputs)\n",
        "\n",
        "# calibrate the model and record the outputs\n",
        "mobilenet.cpu()\n",
        "with torch.autocast(device_type=\"cuda\"):\n",
        "    for i, (image, _) in enumerate(train_loader):\n",
        "        if i >= 128: break\n",
        "        mobilenet(image)\n",
        "\n",
        "# remove the hooks\n",
        "handler.remove_hooks()\n",
        "\n",
        "plot_layer_output_dist(layer_outputs)\n",
        "plot_layer_output_minmax(layer_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sGw7XZSVB5Xn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# remove the hooks\u001b[39;00m\n\u001b[1;32m     22\u001b[0m handler\u001b[38;5;241m.\u001b[39mremove_hooks()\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.10/site-packages/timm/models/vision_transformer.py:704\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 704\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.10/site-packages/timm/models/vision_transformer.py:688\u001b[0m, in \u001b[0;36mVisionTransformer.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    686\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x)\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 688\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.10/site-packages/timm/models/vision_transformer.py:164\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 164\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    165\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))))\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/executorch/lib/python3.10/site-packages/timm/models/vision_transformer.py:91\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_norm(q), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_norm(k)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfused_attn:\n\u001b[0;32m---> 91\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_drop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     q \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# initialize HookHandler and the dictionary to store the outputs\n",
        "handler = HookHandler()\n",
        "layer_outputs = {}\n",
        "\n",
        "# get the layers you want to record\n",
        "# layer_names = [f'blocks.0.attn.qkv', f'blocks.0.mlp.fc1', f'blocks.0.mlp.fc2']\n",
        "layer_names = [name for name, _ in model.named_modules()][1:]\n",
        "block0_layers = get_model_layers(model, match_names=layer_names, match_types=['Linear', 'Dropout', 'LayerNorm', 'GELU'])\n",
        "\n",
        "# create hooks for the layers (<layers>, <hook function>, <dictionary to store the outputs>)\n",
        "handler.create_hooks(block0_layers, get_act_func, layer_outputs)\n",
        "\n",
        "# calibrate the model and record the outputs\n",
        "model.cpu()\n",
        "with torch.autocast(device_type=\"cuda\"):\n",
        "    for i, (image, _) in enumerate(train_loader):\n",
        "        print(i)\n",
        "        if i >= 128: break\n",
        "        model(image)\n",
        "\n",
        "# remove the hooks\n",
        "handler.remove_hooks()\n",
        "\n",
        "# plot_layer_output_dist(layer_outputs)\n",
        "# plot_layer_output_minmax(layer_outputs)\n",
        "\n",
        "# save layer_output as a numpy file\n",
        "np.save('layer_outputs.npy', layer_outputs)\n",
        "\n",
        "# calculate the standard deviation of the outputs, and list the layers with the highest standard deviation\n",
        "layer_std = {}\n",
        "\n",
        "# get the minimum sample size of the layers\n",
        "min_sample_size = min([layer_output.flatten().shape[0] for layer_output in layer_outputs.values()])\n",
        "\n",
        "for layer_name, layer_output in layer_outputs.items():\n",
        "    print(layer_output)\n",
        "\n",
        "    # First, sample each distribution to have the same number of samples\n",
        "    layer_output = layer_output.flatten()\n",
        "    layer_output = layer_output[:min_sample_size]\n",
        "\n",
        "    # calculate the standard deviation of the output\n",
        "    layer_std[layer_name] = np.std(layer_output)\n",
        "    print(f'{layer_name}: {layer_std[layer_name]}')\n",
        "print(layer_std)\n",
        "sorted_layer_std = sorted(layer_std.items(), key=lambda x: x[1], reverse=True)\n",
        "print(sorted_layer_std)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VisionTransformer(\n",
            "  (patch_embed): PatchEmbed(\n",
            "    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
            "    (norm): Identity()\n",
            "  )\n",
            "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "  (patch_drop): Identity()\n",
            "  (norm_pre): Identity()\n",
            "  (blocks): Sequential(\n",
            "    (0): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): LayerScale()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): LayerScale()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (1): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): LayerScale()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): LayerScale()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (2): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): LayerScale()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): LayerScale()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (3): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): LayerScale()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): LayerScale()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (4): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): LayerScale()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): LayerScale()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (5): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): LayerScale()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): LayerScale()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (6): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): LayerScale()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): LayerScale()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (7): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): LayerScale()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): LayerScale()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (8): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): LayerScale()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): LayerScale()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (9): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): LayerScale()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): LayerScale()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (10): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): LayerScale()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): LayerScale()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "    (11): Block(\n",
            "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "        (q_norm): Identity()\n",
            "        (k_norm): Identity()\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls1): LayerScale()\n",
            "      (drop_path1): Identity()\n",
            "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.0, inplace=False)\n",
            "        (norm): Identity()\n",
            "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "        (drop2): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (ls2): LayerScale()\n",
            "      (drop_path2): Identity()\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "  (fc_norm): Identity()\n",
            "  (head_drop): Dropout(p=0.0, inplace=False)\n",
            "  (head): Linear(in_features=384, out_features=100, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)\n",
        "\n",
        "# print number of head and hidden size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "patch_embed\n",
            "patch_embed.proj\n",
            "patch_embed.norm\n",
            "pos_drop\n",
            "patch_drop\n",
            "norm_pre\n",
            "blocks\n",
            "blocks.0\n",
            "blocks.0.norm1\n",
            "blocks.0.attn\n",
            "blocks.0.attn.qkv\n",
            "blocks.0.attn.q_norm\n",
            "blocks.0.attn.k_norm\n",
            "blocks.0.attn.attn_drop\n",
            "blocks.0.attn.proj\n",
            "blocks.0.attn.proj_drop\n",
            "blocks.0.ls1\n",
            "blocks.0.drop_path1\n",
            "blocks.0.norm2\n",
            "blocks.0.mlp\n",
            "blocks.0.mlp.fc1\n",
            "blocks.0.mlp.act\n",
            "blocks.0.mlp.drop1\n",
            "blocks.0.mlp.norm\n",
            "blocks.0.mlp.fc2\n",
            "blocks.0.mlp.drop2\n",
            "blocks.0.ls2\n",
            "blocks.0.drop_path2\n",
            "blocks.1\n",
            "blocks.1.norm1\n",
            "blocks.1.attn\n",
            "blocks.1.attn.qkv\n",
            "blocks.1.attn.q_norm\n",
            "blocks.1.attn.k_norm\n",
            "blocks.1.attn.attn_drop\n",
            "blocks.1.attn.proj\n",
            "blocks.1.attn.proj_drop\n",
            "blocks.1.ls1\n",
            "blocks.1.drop_path1\n",
            "blocks.1.norm2\n",
            "blocks.1.mlp\n",
            "blocks.1.mlp.fc1\n",
            "blocks.1.mlp.act\n",
            "blocks.1.mlp.drop1\n",
            "blocks.1.mlp.norm\n",
            "blocks.1.mlp.fc2\n",
            "blocks.1.mlp.drop2\n",
            "blocks.1.ls2\n",
            "blocks.1.drop_path2\n",
            "blocks.2\n",
            "blocks.2.norm1\n",
            "blocks.2.attn\n",
            "blocks.2.attn.qkv\n",
            "blocks.2.attn.q_norm\n",
            "blocks.2.attn.k_norm\n",
            "blocks.2.attn.attn_drop\n",
            "blocks.2.attn.proj\n",
            "blocks.2.attn.proj_drop\n",
            "blocks.2.ls1\n",
            "blocks.2.drop_path1\n",
            "blocks.2.norm2\n",
            "blocks.2.mlp\n",
            "blocks.2.mlp.fc1\n",
            "blocks.2.mlp.act\n",
            "blocks.2.mlp.drop1\n",
            "blocks.2.mlp.norm\n",
            "blocks.2.mlp.fc2\n",
            "blocks.2.mlp.drop2\n",
            "blocks.2.ls2\n",
            "blocks.2.drop_path2\n",
            "blocks.3\n",
            "blocks.3.norm1\n",
            "blocks.3.attn\n",
            "blocks.3.attn.qkv\n",
            "blocks.3.attn.q_norm\n",
            "blocks.3.attn.k_norm\n",
            "blocks.3.attn.attn_drop\n",
            "blocks.3.attn.proj\n",
            "blocks.3.attn.proj_drop\n",
            "blocks.3.ls1\n",
            "blocks.3.drop_path1\n",
            "blocks.3.norm2\n",
            "blocks.3.mlp\n",
            "blocks.3.mlp.fc1\n",
            "blocks.3.mlp.act\n",
            "blocks.3.mlp.drop1\n",
            "blocks.3.mlp.norm\n",
            "blocks.3.mlp.fc2\n",
            "blocks.3.mlp.drop2\n",
            "blocks.3.ls2\n",
            "blocks.3.drop_path2\n",
            "blocks.4\n",
            "blocks.4.norm1\n",
            "blocks.4.attn\n",
            "blocks.4.attn.qkv\n",
            "blocks.4.attn.q_norm\n",
            "blocks.4.attn.k_norm\n",
            "blocks.4.attn.attn_drop\n",
            "blocks.4.attn.proj\n",
            "blocks.4.attn.proj_drop\n",
            "blocks.4.ls1\n",
            "blocks.4.drop_path1\n",
            "blocks.4.norm2\n",
            "blocks.4.mlp\n",
            "blocks.4.mlp.fc1\n",
            "blocks.4.mlp.act\n",
            "blocks.4.mlp.drop1\n",
            "blocks.4.mlp.norm\n",
            "blocks.4.mlp.fc2\n",
            "blocks.4.mlp.drop2\n",
            "blocks.4.ls2\n",
            "blocks.4.drop_path2\n",
            "blocks.5\n",
            "blocks.5.norm1\n",
            "blocks.5.attn\n",
            "blocks.5.attn.qkv\n",
            "blocks.5.attn.q_norm\n",
            "blocks.5.attn.k_norm\n",
            "blocks.5.attn.attn_drop\n",
            "blocks.5.attn.proj\n",
            "blocks.5.attn.proj_drop\n",
            "blocks.5.ls1\n",
            "blocks.5.drop_path1\n",
            "blocks.5.norm2\n",
            "blocks.5.mlp\n",
            "blocks.5.mlp.fc1\n",
            "blocks.5.mlp.act\n",
            "blocks.5.mlp.drop1\n",
            "blocks.5.mlp.norm\n",
            "blocks.5.mlp.fc2\n",
            "blocks.5.mlp.drop2\n",
            "blocks.5.ls2\n",
            "blocks.5.drop_path2\n",
            "blocks.6\n",
            "blocks.6.norm1\n",
            "blocks.6.attn\n",
            "blocks.6.attn.qkv\n",
            "blocks.6.attn.q_norm\n",
            "blocks.6.attn.k_norm\n",
            "blocks.6.attn.attn_drop\n",
            "blocks.6.attn.proj\n",
            "blocks.6.attn.proj_drop\n",
            "blocks.6.ls1\n",
            "blocks.6.drop_path1\n",
            "blocks.6.norm2\n",
            "blocks.6.mlp\n",
            "blocks.6.mlp.fc1\n",
            "blocks.6.mlp.act\n",
            "blocks.6.mlp.drop1\n",
            "blocks.6.mlp.norm\n",
            "blocks.6.mlp.fc2\n",
            "blocks.6.mlp.drop2\n",
            "blocks.6.ls2\n",
            "blocks.6.drop_path2\n",
            "blocks.7\n",
            "blocks.7.norm1\n",
            "blocks.7.attn\n",
            "blocks.7.attn.qkv\n",
            "blocks.7.attn.q_norm\n",
            "blocks.7.attn.k_norm\n",
            "blocks.7.attn.attn_drop\n",
            "blocks.7.attn.proj\n",
            "blocks.7.attn.proj_drop\n",
            "blocks.7.ls1\n",
            "blocks.7.drop_path1\n",
            "blocks.7.norm2\n",
            "blocks.7.mlp\n",
            "blocks.7.mlp.fc1\n",
            "blocks.7.mlp.act\n",
            "blocks.7.mlp.drop1\n",
            "blocks.7.mlp.norm\n",
            "blocks.7.mlp.fc2\n",
            "blocks.7.mlp.drop2\n",
            "blocks.7.ls2\n",
            "blocks.7.drop_path2\n",
            "blocks.8\n",
            "blocks.8.norm1\n",
            "blocks.8.attn\n",
            "blocks.8.attn.qkv\n",
            "blocks.8.attn.q_norm\n",
            "blocks.8.attn.k_norm\n",
            "blocks.8.attn.attn_drop\n",
            "blocks.8.attn.proj\n",
            "blocks.8.attn.proj_drop\n",
            "blocks.8.ls1\n",
            "blocks.8.drop_path1\n",
            "blocks.8.norm2\n",
            "blocks.8.mlp\n",
            "blocks.8.mlp.fc1\n",
            "blocks.8.mlp.act\n",
            "blocks.8.mlp.drop1\n",
            "blocks.8.mlp.norm\n",
            "blocks.8.mlp.fc2\n",
            "blocks.8.mlp.drop2\n",
            "blocks.8.ls2\n",
            "blocks.8.drop_path2\n",
            "blocks.9\n",
            "blocks.9.norm1\n",
            "blocks.9.attn\n",
            "blocks.9.attn.qkv\n",
            "blocks.9.attn.q_norm\n",
            "blocks.9.attn.k_norm\n",
            "blocks.9.attn.attn_drop\n",
            "blocks.9.attn.proj\n",
            "blocks.9.attn.proj_drop\n",
            "blocks.9.ls1\n",
            "blocks.9.drop_path1\n",
            "blocks.9.norm2\n",
            "blocks.9.mlp\n",
            "blocks.9.mlp.fc1\n",
            "blocks.9.mlp.act\n",
            "blocks.9.mlp.drop1\n",
            "blocks.9.mlp.norm\n",
            "blocks.9.mlp.fc2\n",
            "blocks.9.mlp.drop2\n",
            "blocks.9.ls2\n",
            "blocks.9.drop_path2\n",
            "blocks.10\n",
            "blocks.10.norm1\n",
            "blocks.10.attn\n",
            "blocks.10.attn.qkv\n",
            "blocks.10.attn.q_norm\n",
            "blocks.10.attn.k_norm\n",
            "blocks.10.attn.attn_drop\n",
            "blocks.10.attn.proj\n",
            "blocks.10.attn.proj_drop\n",
            "blocks.10.ls1\n",
            "blocks.10.drop_path1\n",
            "blocks.10.norm2\n",
            "blocks.10.mlp\n",
            "blocks.10.mlp.fc1\n",
            "blocks.10.mlp.act\n",
            "blocks.10.mlp.drop1\n",
            "blocks.10.mlp.norm\n",
            "blocks.10.mlp.fc2\n",
            "blocks.10.mlp.drop2\n",
            "blocks.10.ls2\n",
            "blocks.10.drop_path2\n",
            "blocks.11\n",
            "blocks.11.norm1\n",
            "blocks.11.attn\n",
            "blocks.11.attn.qkv\n",
            "blocks.11.attn.q_norm\n",
            "blocks.11.attn.k_norm\n",
            "blocks.11.attn.attn_drop\n",
            "blocks.11.attn.proj\n",
            "blocks.11.attn.proj_drop\n",
            "blocks.11.ls1\n",
            "blocks.11.drop_path1\n",
            "blocks.11.norm2\n",
            "blocks.11.mlp\n",
            "blocks.11.mlp.fc1\n",
            "blocks.11.mlp.act\n",
            "blocks.11.mlp.drop1\n",
            "blocks.11.mlp.norm\n",
            "blocks.11.mlp.fc2\n",
            "blocks.11.mlp.drop2\n",
            "blocks.11.ls2\n",
            "blocks.11.drop_path2\n",
            "norm\n",
            "fc_norm\n",
            "head_drop\n",
            "head\n"
          ]
        }
      ],
      "source": [
        "for name, module in model.named_modules():\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Number of parameters in each layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_num_parameters_distribution(model):\n",
        "    num_parameters = dict()\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.dim() > 1:\n",
        "            num_parameters[name] = param.numel()\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "    plt.grid(axis='y')\n",
        "    plt.bar(list(num_parameters.keys()), list(num_parameters.values()))\n",
        "    plt.title('#Parameter Distribution')\n",
        "    plt.ylabel('Number of Parameters')\n",
        "    plt.xticks(rotation=60)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_num_parameters_distribution(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Weight distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def get_quantized_range(bitwidth):\n",
        "    quantized_max = (1 << (bitwidth - 1)) - 1\n",
        "    quantized_min = -(1 << (bitwidth - 1))\n",
        "    return quantized_min, quantized_max\n",
        "\n",
        "def plot_weight_distribution(model, bitwidth=32):\n",
        "    # bins = (1 << bitwidth) if bitwidth <= 8 else 256\n",
        "    if bitwidth <= 8:\n",
        "        qmin, qmax = get_quantized_range(bitwidth)\n",
        "        bins = np.arange(qmin, qmax + 2)\n",
        "        align = 'left'\n",
        "    else:\n",
        "        bins = 256\n",
        "        align = 'mid'\n",
        "    fig, axes = plt.subplots(9,6, figsize=(20, 15))\n",
        "    axes = axes.ravel()\n",
        "    plot_index = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.dim() > 1:\n",
        "            ax = axes[plot_index]\n",
        "            ax.hist(param.detach().view(-1).cpu(), bins=bins, density=True,\n",
        "                    align=align, color = 'blue', alpha = 0.5,\n",
        "                    edgecolor='black' if bitwidth <= 4 else None)\n",
        "            if bitwidth <= 4:\n",
        "                quantized_min, quantized_max = get_quantized_range(bitwidth)\n",
        "                ax.set_xticks(np.arange(start=quantized_min, stop=quantized_max+1))\n",
        "            ax.set_xlabel(name)\n",
        "            ax.set_ylabel('density')\n",
        "            plot_index += 1\n",
        "    fig.suptitle(f'Histogram of Weights (bitwidth={bitwidth} bits)')\n",
        "    fig.tight_layout()\n",
        "    fig.subplots_adjust(top=0.925)\n",
        "    plt.savefig(os.path.join('weight_distribution.png'))\n",
        "\n",
        "plot_weight_distribution(model)\n",
        "plot_weight_distribution(mobilenet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bztBdOxJRKTD"
      },
      "source": [
        "## To ignore specific layers for quantization:\n",
        "([xnnpack source code](https://github.com/pytorch/pytorch/blob/main/torch/ao/quantization/quantizer/xnnpack_quantizer.py\\), also you can try messing around with the Observer classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kdAyjLwXQlya"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['blocks.0.norm1', 'blocks.0.norm2', 'blocks.1.norm1', 'blocks.1.norm2', 'blocks.2.norm1', 'blocks.2.norm2', 'blocks.3.norm1', 'blocks.3.norm2', 'blocks.4.norm1', 'blocks.4.norm2', 'blocks.5.norm1', 'blocks.5.norm2', 'blocks.6.norm1', 'blocks.6.norm2', 'blocks.7.norm1', 'blocks.7.norm2', 'blocks.8.norm1', 'blocks.8.norm2', 'blocks.9.norm1', 'blocks.9.norm2', 'blocks.10.norm1', 'blocks.10.norm2', 'blocks.11.norm1', 'blocks.11.norm2']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<__main__.PartialXNNPACKQuantizer at 0x7f3c7ceb13f0>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Replace functions XNNPackQuantizer uses for annotation.\n",
        "\n",
        "def get_module_names(name):\n",
        "    names = name.split(\".\")\n",
        "    return [\".\".join(names[i:]) for i in reversed(range(len(names)))]\n",
        "\n",
        "def parse_string(name):\n",
        "    if name.startswith(\"L\"): return name[10:]\n",
        "    split_getattr = name.split(\")\")\n",
        "    ig_left = split_getattr[0].split(\"L['self'].\")[-1].split(\",\")[0]\n",
        "    ig_right = split_getattr[0].split(\", '\")[-1][:-1]\n",
        "    return ig_left + \".\" + str(ig_right) + split_getattr[1]\n",
        "\n",
        "def is_name_in_ignore_list(name, IGNORE_LIST):\n",
        "    return name in IGNORE_LIST\n",
        "\n",
        "def name_not_in_ignore_list(n, IGNORE_LIST) -> bool:\n",
        "    nn_module_stack = n.meta.get(\"nn_module_stack\", {})\n",
        "    names = [n for n, klass in nn_module_stack.values()]\n",
        "    if len(names) == 0:\n",
        "        return True\n",
        "\n",
        "    names = get_module_names(parse_string(names[-1]))\n",
        "    set1 = set(names)\n",
        "    set2 = set(IGNORE_LIST)\n",
        "    # if len(set1.intersection(set2)) == 0:\n",
        "    #     print(\"DEBUG: \", names)\n",
        "    return len(set1.intersection(set2)) == 0\n",
        "\n",
        "def get_module_name_filter(module_name: str, IGNORE_LIST):\n",
        "    def module_name_filter(n) -> bool:\n",
        "        nn_module_stack = n.meta.get(\"nn_module_stack\", {})\n",
        "        names = [n for n, klass in nn_module_stack.values()]\n",
        "        if len(names) == 0:\n",
        "            return False\n",
        "\n",
        "        names = get_module_names(parse_string(names[-1]))\n",
        "        return (module_name in names) and name_not_in_ignore_list(n, IGNORE_LIST)\n",
        "    return module_name_filter\n",
        "\n",
        "\n",
        "def get_module_type_filter(tp, IGNORE_LIST):\n",
        "    def module_type_filter(n) -> bool:\n",
        "        nn_module_stack = n.meta.get(\"nn_module_stack\", {})\n",
        "        types = [t for _, t in nn_module_stack.values()]\n",
        "        return (tp in types) and name_not_in_ignore_list(n, IGNORE_LIST)\n",
        "\n",
        "    return module_type_filter\n",
        "\n",
        "\n",
        "def get_not_module_type_or_name_filter(\n",
        "    tp_list, module_name_list, IGNORE_LIST\n",
        "):\n",
        "    module_type_filters = [get_module_type_filter(tp) for tp in tp_list]\n",
        "    module_name_list_filters = [get_module_name_filter(m) for m in module_name_list]\n",
        "\n",
        "    def not_module_type_or_name_filter(n) -> bool:\n",
        "        return not any(f(n) for f in module_type_filters + module_name_list_filters) and name_not_in_ignore_list(n, IGNORE_LIST)\n",
        "\n",
        "    return not_module_type_or_name_filter\n",
        "\n",
        "class PartialXNNPACKQuantizer(XNNPACKQuantizer): # skips quantizing layers inside the ignore_list\n",
        "    def __init__(self, ignore_list):\n",
        "        super().__init__()\n",
        "        self.ignore_list = ignore_list\n",
        "\n",
        "    def _annotate_for_static_quantization_config(\n",
        "        self, model: torch.fx.GraphModule\n",
        "    ) -> torch.fx.GraphModule:\n",
        "        print(\"annotating for static quantization\")\n",
        "        module_name_list = list(self.module_name_config.keys())\n",
        "        for module_name, config in self.module_name_config.items():\n",
        "            self._annotate_all_static_patterns(\n",
        "                model, config, get_module_name_filter(module_name, self.ignore_list)\n",
        "            )\n",
        "\n",
        "        tp_list = list(self.module_type_config.keys())\n",
        "        for module_type, config in self.module_type_config.items():\n",
        "            self._annotate_all_static_patterns(\n",
        "                model, config, get_module_type_filter(module_type, self.ignore_list)\n",
        "            )\n",
        "\n",
        "        self._annotate_all_static_patterns(\n",
        "            model,\n",
        "            self.global_config,\n",
        "            get_not_module_type_or_name_filter(tp_list, module_name_list, self.ignore_list),\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def _annotate_for_dynamic_quantization_config(\n",
        "        self, model: torch.fx.GraphModule\n",
        "    ) -> torch.fx.GraphModule:\n",
        "        print(\"annotating for dynamic quantization\")\n",
        "        module_name_list = list(self.module_name_config.keys())\n",
        "        for module_name, config in self.module_name_config.items():\n",
        "            self._annotate_all_dynamic_patterns(\n",
        "                model, config, get_module_name_filter(module_name, self.ignore_list)\n",
        "            )\n",
        "\n",
        "        tp_list = list(self.module_type_config.keys())\n",
        "        for module_type, config in self.module_type_config.items():\n",
        "            self._annotate_all_dynamic_patterns(\n",
        "                model, config, get_module_type_filter(module_type, self.ignore_list)\n",
        "            )\n",
        "\n",
        "        self._annotate_all_dynamic_patterns(\n",
        "            model,\n",
        "            self.global_config,\n",
        "            get_not_module_type_or_name_filter(tp_list, module_name_list, self.ignore_list),\n",
        "        )\n",
        "        return model\n",
        "\n",
        "# quantizer = XNNPACKQuantizer()\n",
        "\n",
        "act_list = [name for name, _ in model.named_modules() if 'act' in name]\n",
        "fc_list = [name for name, _ in model.named_modules() if 'fc' in name]\n",
        "qkv_list = [name for name, _ in model.named_modules() if 'qkv' in name]\n",
        "layernorm_list = [name for name, _ in model.named_modules() if ('norm1' in name) or ('norm2' in name)]\n",
        "layerscale_list = [name for name, _ in model.named_modules() if 'ls' in name]\n",
        "ignore_list = layernorm_list\n",
        "print(ignore_list)\n",
        "quantizer = PartialXNNPACKQuantizer(ignore_list=ignore_list) # replace XNNPACKQuantizer()\n",
        "quantizer.set_global(get_symmetric_quantization_config(is_per_channel=False, is_qat=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size (MB) before quantization: 86.903222\n",
            "Accuracy of the model on the test images: 93.6%\n",
            "ignore_list: ['blocks.0.norm1', 'blocks.0.norm2', 'blocks.1.norm1', 'blocks.1.norm2', 'blocks.2.norm1', 'blocks.2.norm2', 'blocks.3.norm1', 'blocks.3.norm2', 'blocks.4.norm1', 'blocks.4.norm2', 'blocks.5.norm1', 'blocks.5.norm2', 'blocks.6.norm1', 'blocks.6.norm2', 'blocks.7.norm1', 'blocks.7.norm2', 'blocks.8.norm1', 'blocks.8.norm2', 'blocks.9.norm1', 'blocks.9.norm2', 'blocks.10.norm1', 'blocks.10.norm2', 'blocks.11.norm1', 'blocks.11.norm2', 'pos_drop', 'head_drop', 'norm', 'head', 'blocks.9.mlp.fc1']\n",
            "Quantizing model...\n",
            "annotating for static quantization\n",
            "Exporting model...\n",
            "Evaluating model...\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:27<00:00, 17.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the test images: 87.4%\n",
            "Model Size: 23.82 MB\n",
            "Accuracy: 87.40%\n",
            "Execution Time: 27.93 s\n",
            "Model Score: 28.00\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# evaluate before quantization\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# acc = evaluate_model(model, mini_test_dataset, device)\n",
        "# print('Size (MB) before quantization:', get_size_of_model(model))\n",
        "# print(f'Accuracy of the model on the test images: {acc}%') # 92.8%\n",
        "print('Size (MB) before quantization:', '86.903222')\n",
        "print(f'Accuracy of the model on the test images: {93.6}%') # 92.8%\n",
        "\n",
        "\n",
        "act_list = [name for name, _ in model.named_modules() if 'act' in name]\n",
        "fc_list = [name for name, _ in model.named_modules() if 'fc' in name]\n",
        "qkv_list = [name for name, _ in model.named_modules() if 'qkv' in name]\n",
        "layernorm_list = [name for name, _ in model.named_modules() if ('norm1' in name) or ('norm2' in name)]\n",
        "layerscale_list = [name for name, _ in model.named_modules() if 'ls' in name]\n",
        "proj_list = [name for name, _ in model.named_modules() if 'proj' in name]\n",
        "\n",
        "# ignore_list = [\"blocks.9.mlp.fc1\", \"blocks.2.attn.qkv\", \"blocks.10.mlp.fc1\"]\n",
        "ignore_list = layernorm_list + ['pos_drop', 'head_drop', 'norm', 'head', 'blocks.9.mlp.fc1']\n",
        "# ignore_list = ignore_list + ['blocks.9.mlp.fc1', 'blocks.10.mlp.fc1']\n",
        "\n",
        "print(f'ignore_list: {ignore_list}')\n",
        "\n",
        "quantizer = PartialXNNPACKQuantizer(ignore_list=ignore_list) # replace XNNPACKQuantizer()\n",
        "quantizer.set_global(get_symmetric_quantization_config(is_per_channel=False, is_qat=False))\n",
        "\n",
        "\n",
        "# quantize model\n",
        "print('Quantizing model...')\n",
        "model = torch.load('0.9099_deit3_small_patch16_224.pth', map_location='cpu')\n",
        "model.cpu()\n",
        "quantized_model = quantize_ptq_model(model, train_loader, per_channel=False, quantizer=quantizer)\n",
        "# quantized_model = quantize_ptq_model(model, train_loader, per_channel=False)\n",
        "torch.ao.quantization.move_exported_model_to_eval(quantized_model)\n",
        "\n",
        "print('Exporting model...')\n",
        "quantized_model_path = \"deits_quantized.pth\"\n",
        "\n",
        "quantized_model.cpu()\n",
        "cpu_example_inputs = (torch.randn([1, 3, 224, 224]), ) # batch_size should equal to 1 on inference.\n",
        "quantized_ep = torch.export.export(quantized_model, cpu_example_inputs)\n",
        "torch.export.save(quantized_ep, quantized_model_path)\n",
        "\n",
        "print('Evaluating model...')\n",
        "lab4_cifar100_evaluation(quantized_model_path) # 84.4%\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:27<00:00, 18.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the test images: 87.0%\n",
            "Model Size: 22.05 MB\n",
            "Accuracy: 87.00%\n",
            "Execution Time: 27.46 s\n",
            "Model Score: 30.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lab4_cifar100_evaluation(\"deits_quantized_87_pos_drop_head_drop_norm_head_mlp.pth\") # 84.4%"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Ou1NYqlNIoXf",
        "V_xk1sSXoqsJ",
        "NO1-XuLJBkL2",
        "bztBdOxJRKTD"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
