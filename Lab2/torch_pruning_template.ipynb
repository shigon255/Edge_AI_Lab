{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdC9V9xB10Gt"
      },
      "source": [
        "# **Template for Torch-Pruning**\n",
        "\n",
        "This template is just built for your convinience.\n",
        "\n",
        "You are not required to follow the steps and method given below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch_pruning"
      ],
      "metadata": {
        "id": "7jfw5rNimfwu",
        "outputId": "b5464a44-1f8b-4a2a-9a02-5ace83b35c3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_pruning\n",
            "  Downloading torch_pruning-1.3.7-py3-none-any.whl (56 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torch_pruning) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_pruning) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torch_pruning) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->torch_pruning)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->torch_pruning)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->torch_pruning)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->torch_pruning)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->torch_pruning)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.4/410.6 MB\u001b[0m \u001b[31m127.8 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models import mobilenet_v2\n",
        "import torch_pruning as tp"
      ],
      "metadata": {
        "id": "-LiF8Vrqm2FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A Minimal Example ##  \n",
        "In this section, you will perform channel pruning using the library [Torch-Pruning](https://github.com/VainF/Torch-Pruning).  \n",
        "\n",
        "The puuner in Torch-Pruning has three main functions: sparse training (optional), importance estimation, and parameter removal.  \n",
        "Torch-pruning offers two core features to support this process:\n",
        "\n",
        "tp.importance(): This criteria is utilized to measure the importance of weights.  \n",
        "\n",
        "tp.pruner(): This is a pruner used for the actual pruning of the parameters.  \n",
        "\n",
        "For detailed information on this process, please refer to this [tutorial](https://github.com/VainF/Torch-Pruning/wiki/4.-High%E2%80%90level-Pruners/). Additionally, a more practical example is available in [here](https://github.com/VainF/Torch-Pruning/blob/master/benchmarks/main.py)."
      ],
      "metadata": {
        "id": "dQ411pYQhkw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Load model\n"
      ],
      "metadata": {
        "id": "s9e8CpBhmph9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('./mobilenetv2_0.963.pth', map_location=\"cpu\")\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "umaWm4eYms6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Prepare a pruner\n",
        "By default, Torch-Pruning will automatically prune the last non-singleton dim of these parameters. If you want to customize this behaviour, please provide an `unwrapped_parameters` list as the following example."
      ],
      "metadata": {
        "id": "mSe6NDqqm_qN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importance criterion\n",
        "imp = tp.importance.GroupNormImportance(p=2) # or GroupTaylorImportance(), GroupHessianImportance(), etc.\n",
        "\n",
        "# Initialize a pruner with the model and the importance criterion\n",
        "example_inputs = torch.randn(1, 3, 224, 224).to(device)\n",
        "\n",
        "ignored_layers = []\n",
        "for m in model.modules():\n",
        "  if isinstance(m, torch.nn.Linear) and m.out_features == 10: # ignore the classifier\n",
        "    ignored_layers.append(m)\n",
        "\n",
        "pruner = tp.pruner.GroupNormPruner ( # you can choose any pruner you like.\n",
        "    model,\n",
        "    example_inputs,\n",
        "    importance=imp,   # Importance Estimator\n",
        "    pruning_ratio=0.5, # remove 50% channels, ex :ResNet18 = {64, 128, 256, 512} => ResNet18_Half = {32, 64, 128, 256}\n",
        "    ignored_layers=ignored_layers,\n",
        "    # pruning_ratio_dict = {model.conv1: 0.2}, # manually set the sparsity of model.conv1\n",
        "    iterative_steps = 1,  # number of steps to achieve the target ch_sparsity.\n",
        "    ignored_layers = ignored_layers,        # ignore some layers such as the finall linear classifier\n",
        "    channel_groups = channel_groups,  # round channels\n",
        ")"
      ],
      "metadata": {
        "id": "up4Y3mqCnEIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Prune the model"
      ],
      "metadata": {
        "id": "d1XmDH9ToKYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model size before pruning\n",
        "base_macs, base_nparams = tp.utils.count_ops_and_params(model, torch.randn(1,3,224,224))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "if isinstance(imp, tp.importance.GroupTaylorImportance):\n",
        "  # Taylor expansion requires gradients for importance estimation\n",
        "  loss = model(example_inputs).sum() # A dummy loss, please replace this line with your loss function and data!\n",
        "  loss.backward() # before pruner.step()\n",
        "\n",
        "# prune\n",
        "pruner.step()\n",
        "\n",
        "# Parameter & MACs Counter\n",
        "pruned_macs, pruned_nparams = tp.utils.count_ops_and_params(model, torch.randn(1,3,224,224))\n",
        "MFLOPs = pruned_macs/1e6\n",
        "print(\"The pruned model:\")\n",
        "print(model)\n",
        "print(\"Summary:\")\n",
        "print(\"MFLOPs: \")\n",
        "print(MFLOPs)\n",
        "\n",
        "# finetune the pruned model here\n",
        "# finetune(model)\n",
        "# ..."
      ],
      "metadata": {
        "id": "qmklPY2goMX2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}